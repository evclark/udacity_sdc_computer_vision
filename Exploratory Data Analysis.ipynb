{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the dataset\n",
    "\n",
    "\n",
    "In this notebook, we will perform an EDA (Exploratory Data Analysis) on the processed Waymo dataset (data in the `processed` folder). In the first part, you will create a function to display "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "tf.get_logger().setLevel(\"ERROR\") #Turn off annoying tf INFO messages \n",
    "from tqdm.notebook import trange\n",
    "from IPython import display\n",
    "from collections import Counter\n",
    "\n",
    "from utils import get_dataset\n",
    "\n",
    "import exploratory_data_analysis as eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up matplotlib\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [18, 12]\n",
    "\n",
    "#Set the random seed so we can produce reproducible results when we want them\n",
    "np.random.seed(1)\n",
    "\n",
    "#Initialize GroundTruthAnnotator we will use for the exercises below\n",
    "visualizer = eda.GroundTruthAnnotator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a function to display an image and the bounding boxes\n",
    "\n",
    "Implement the `display_instances` function below. This function takes a batch as an input and display an image with its corresponding bounding boxes. The only requirement is that the classes should be color coded (eg, vehicles in red, pedestrians in blue, cyclist in green)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STUDENT SOLUTION HERE\n",
    "# See GroundTruthAnnotator.batch_annotate_ground_truth() in\n",
    "# exploratory_data_analysis.py, which completes the same functionality as\n",
    "# display_instances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display 10 images \n",
    "\n",
    "Using the dataset created in the second cell and the function you just coded, display 10 random images with the associated bounding boxes. You can use the methods `take` and `shuffle` on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STUDENT SOLUTION HERE\n",
    "dataset = get_dataset(\"data/train/segment-1005081002024129653_5313_150_5333_150_with_camera_labels.tfrecord\")\n",
    "dataset = dataset.shuffle(10)\n",
    "batch = dataset.take(10)\n",
    "mosiac_img_path = visualizer.batch_annotate_ground_truth(batch)\n",
    "print(mosiac_img_path)\n",
    "img = cv2.imread(mosiac_img_path)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional EDA\n",
    "\n",
    "In this last part, you are free to perform any additional analysis of the dataset. What else would like to know about the data?\n",
    "For example, think about data distribution. So far, you have only looked at a single file..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-------- STUDENT SOLUTION BELOW --------**\n",
    "\n",
    "### Compute Statistical Analysis on Dataset\n",
    "\n",
    "First, let's process the provided data to decide how we want to split it into training and validation sets. Let's pull a batch of images from each tf record file, annotate the images with ground truth labels, and calculate various statistics about class frequency, physical characteristics, image characteristics, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "\n",
    "tfrecord_paths = glob.glob(\"data/train/*.tfrecord\")\n",
    "\n",
    "mosaic_paths = []\n",
    "train_and_val_stats = eda.StatisticsAggregator()\n",
    "\n",
    "for tfrecord_idx in trange(len(tfrecord_paths)):\n",
    "    #Get a batch from this tfrecord file\n",
    "    tfrecord_path = tfrecord_paths[tfrecord_idx]\n",
    "    dataset = get_dataset(tfrecord_path)\n",
    "    dataset = dataset.shuffle(BATCH_SIZE)\n",
    "    batch = dataset.take(BATCH_SIZE)\n",
    "\n",
    "    #Create a ground truth visualization mosaic for this batch\n",
    "    basename = os.path.basename(tfrecord_path)\n",
    "    mosaic_path = visualizer.batch_annotate_ground_truth(batch, basename)\n",
    "    mosaic_paths.append(mosaic_path) \n",
    "\n",
    "    #Calculate statistics for this batch and add to the aggregated statistics\n",
    "    #for the combined training and validation sets (we will figure out how we\n",
    "    #want to split them later)\n",
    "    batch = list(batch.as_numpy_iterator()) #FIXME: call this in StatisticsAggregator\n",
    "    for elem in batch:\n",
    "        train_and_val_stats.calculateAllStats(batch)\n",
    "\n",
    "#Save out the min, median, and max images of the inspected dataset for various\n",
    "#image attributes. FIXME: remove\n",
    "train_and_val_stats.img_examples[\"mean_brightness\"].saveExampleImages(\"mean_brightness\")\n",
    "train_and_val_stats.img_examples[\"contrast\"].saveExampleImages(\"contrast\")\n",
    "train_and_val_stats.img_examples[\"sharpness\"].saveExampleImages(\"sharpness\")\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Image Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will analyze the visual characteristics of the images in the dataset, without regards to the classes in the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's just get a quick qualitative visual idea of what the dataset looks like. We'll display annotated mosaics for 10 batches of images (images in one batch all come from the same trip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_to_show = np.random.choice(mosaic_paths, 10)\n",
    "for img_path in imgs_to_show:\n",
    "    img = cv2.imread(img_path)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's get an idea of the range of images encountered across several different attributes such as image mean brightness, image contrast, and image sharpness. This will give us insight into the range of visual conditions encountered while driving (e.g. dark because of night, low contrast because of fog, blurry because of raindrops on the lens or motion blur). We will display the images representing the minimum, median, and maximum for each attribute in order to get a visual qualitative sense of the range which exists in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attribute_name, example in train_and_val_stats.img_examples.items():\n",
    "    imgs = [example.min_img, example.median_img, example.max_img]\n",
    "    values = {\"min\": example.min_value, \n",
    "              \"median\": example.median_value, \n",
    "              \"max\": example.max_value}\n",
    "    fig, axes = plt.subplots(1, 3)\n",
    "    for j, key in enumerate(values.keys()):\n",
    "        axes[j].imshow(imgs[j])\n",
    "        axes[j].axis(\"off\")\n",
    "        axes[j].set_title(\"%s %s = %.0f\" % (key, attribute_name, values[key]))\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's get a more complete quantitative insight into the brightness, sharpness, and contrast across the whole dataset by plotting histograms for each attribute: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for attribute_name in train_and_val_stats.img_attributes:\n",
    "    plt.title(\"img \" + attribute_name)\n",
    "    plt.hist(train_and_val_stats.img_statistics[attribute_name], bins=\"auto\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Class Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will analyze the characteristics of the ground truth classes found in the images, e.g. how frequently each class appears, its size, and where it tends to appear in the image.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Frequency\n",
    "\n",
    "First, let's get a sense of how common each class is in the images by plotting a histogram of class frequency per image for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_idx in eda.CLASS_TO_LABEL_MAP.keys():\n",
    "    bins = [i for i in range(max(train_and_val_stats.class_freqs[class_idx]))]\n",
    "    plt.hist(train_and_val_stats.class_freqs[class_idx], bins, log=True)\n",
    "    label = eda.CLASS_TO_LABEL_MAP[class_idx]\n",
    "    plt.title(\"%s frequency histogram\" % label)\n",
    "    plt.xlabel(\"number of %ss in image\" % label)\n",
    "    plt.ylabel(\"frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Size\n",
    "\n",
    "Next, let's get a sense of how large the classes appear in the images, by plotting a histogram of thier bounding box widths and heights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for class_idx in eda.CLASS_TO_LABEL_MAP.keys():\n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "    axes[0].hist(train_and_val_stats.class_widths[class_idx], bins=\"auto\", log=True)\n",
    "    axes[0].set_title(\"width\")\n",
    "    axes[0].set_ylabel(\"frequency\")\n",
    "\n",
    "    axes[1].hist(train_and_val_stats.class_heights[class_idx], bins=\"auto\", log=True)\n",
    "    axes[1].set_title(\"height\")\n",
    "\n",
    "    plt.suptitle(\"%s size\" % eda.CLASS_TO_LABEL_MAP[class_idx])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Locations in Images\n",
    "\n",
    "Finally, let's get a sense of where the classes tend to be located in the images by plotting a scatter plot of the bounding box centers. We will let the points be semi-transparent to help us get a sense of density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for class_idx in eda.CLASS_TO_LABEL_MAP.keys():\n",
    "    xs, ys = zip(*train_and_val_stats.class_bounding_box_centers[class_idx])\n",
    "\n",
    "    plt.scatter(xs, ys, alpha=0.01)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    label = eda.CLASS_TO_LABEL_MAP[class_idx]\n",
    "    plt.title(\"%s locations in image\" % label)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d01fab7a133ae62d4756cc359b975753c78e8eb65a2eecc2a31f88e511a59ed5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
